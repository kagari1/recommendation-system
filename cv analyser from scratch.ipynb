{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee3b9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ac8378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d0399fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv_aida_bouanis.pdf\n"
     ]
    }
   ],
   "source": [
    "filed=input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2390aafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xref table not zero-indexed. ID numbers for objects will be corrected.\n"
     ]
    }
   ],
   "source": [
    "with open(filed, \"rb\") as pdf_file:\n",
    "        \n",
    "        read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "        number_of_pages = read_pdf.getNumPages()\n",
    "        page = read_pdf.pages[0]\n",
    "        text = page.extractText() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd72495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIDA BOUANIS\n",
      "Data & Software Engineering Student\n",
      "@a.bouanis@insea.ac.ma /phone+212 695-05-7093 /map♂mar¶erLaayoune, Morocco /linkedinaida-bouanis\n",
      "EDUCATION\n",
      "Engineer’s Degree. in Data & Software Engineering\n",
      "National Institute of Statistics and Applied Economics (INSEA)\n",
      "Ὄ5Oct 2020 – Ongoing /map♂mar¶erRabat, Morocco\n",
      "A.S. in Mathematics and Physics\n",
      "Lissane Eddine Preparatory Classes Center\n",
      "Ὄ52018 – 2020 /map♂mar¶erLaayoune ,Morocco\n",
      "High School Diploma in Mathematics Science Option A\n",
      "with Honors\n",
      "Bab Sahra High School\n",
      "Ὄ52015 – 2018 /map♂mar¶erGuelmim,Morocco\n",
      "EXPERIENCE\n",
      "Data & Software Engineering Student (Projects)\n",
      "INSEA\n",
      "Ὄ5Oct 2020 – Ongoing /map♂mar¶erRabat, Morocco\n",
      "•I have designed a dashboard for a dentist cabinet in order to\n",
      "keep track of the patients using PHP,HTML ,JavaScript andCSS.\n",
      "•I have written a statistical report on the distribution of of salaries\n",
      "in a company using Pandas ,Matplotlib andNumPy .\n",
      "•I have developed a memory game using Java, and Swing .\n",
      "•I have developed a mini C compiler using Python oriented ob-\n",
      "ject.\n",
      "•I have built a machine learning model for customer classiﬁcation\n",
      "using Python libraries .\n",
      "Internship discovery (Projects)\n",
      "Guelmim-Oued Noun region\n",
      "Ὄ5July 2021 – August 2021 /map♂mar¶erGuelmim, Morocco\n",
      "•I have developed the front end of a website for the regional\n",
      "council using PHP,HTML ,JavaScript andCSS.MOST PROUD OF\n",
      "♀Leadership spirit\n",
      "Always work as a member of the team\n",
      "and guide them to work as one when\n",
      "in charge\n",
      "STRENGTHS\n",
      "Programmation :\n",
      "Python (Pandas,Dash, Plotly, Seaborn, Numpy...)\n",
      "JAVA JEE / SPRING PHP C SAS\n",
      "LISP PROLOG R\n",
      "Database :\n",
      "Mysql Oracle PL/SQL\n",
      "Cloud Computing and virtualisation :\n",
      "Docker PAAS SAAS\n",
      "Operating system :\n",
      "Windows Linux/Unix\n",
      "Applied mathematics :\n",
      "Descriptive & inferential statistics\n",
      "Linear regression dimension reduction\n",
      "Tools & others :\n",
      "Power Bi Airﬂow UML/Merise Apex\n",
      "Android studio HTML/CSS\n",
      "Data Scrapping & Crowling\n",
      "CERTIFICATES\n",
      "Fundamentals of Deep Learning - NVIDIA.\n",
      "Database Programming with SQL - Oracle.\n",
      "LANGUAGES\n",
      "Arabic Native\n",
      "English Proﬁcient\n",
      "French Proﬁcient\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69f9610f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_mobile_number(text):\n",
    "    phone = re.findall(\"(/^(?:(?:(?:\\+|00)212[\\s]?(?:[\\s]?\\(0\\)[\\s]?)?)|0){1}(?:5[\\s.-]?[2-3]|6[\\s.-]?[13-9]){1}[0-9]{1}(?:[\\s.-]?\\d{2}){3}$)\",text)\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "        if len(number) > 10:\n",
    "            return '+' + number\n",
    "        else:\n",
    "            return number\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cab6030",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_pattern=\"/^(?:(?:(?:\\+|00)212[\\s]?(?:[\\s]?\\(0\\)[\\s]?)?)|0){1}(?:5[\\s.-]?[2-3]|6[\\s.-]?[13-9]){1}[0-9]{1}(?:[\\s.-]?\\d{2}){3}$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90d30429",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_mobile_number(text)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "741e6221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_email(email):\n",
    "    email = re.findall(\"([^@|\\s]+@[^@]+\\.[^@|\\s]+)\", email)\n",
    "    if email:\n",
    "        try:\n",
    "            return email[0].split()[0].strip(';')\n",
    "        except IndexError:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f69eea76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a.bouanis@insea.ac.ma'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_email(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51c67fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# load pre-trained model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "SKILLS_DB = [\n",
    "    'machine learning',\n",
    "    'data science',\n",
    "    'python',\n",
    "    'word',\n",
    "    'excel',\n",
    "    'English',\n",
    "    'Data Entry',\n",
    "  'Billing',\n",
    "'Scheduling',\n",
    "'Microsoft Office Skills',\n",
    "'Office Equipment',\n",
    "'QuickBooks',\n",
    "'Salesforce',\n",
    "'Calendar Management',\n",
    "'Computer Skills',\n",
    "'Communication',\n",
    "'Product Knowledge',\n",
    "'Lead Qualification',\n",
    "'Lead Prospecting' ,\n",
    "'Customer Needs Analysis',\n",
    "'CRM Software (Salesforce, Hubspot, Zoho, Freshsales)',\n",
    "    'Deep Learning'\n",
    "  ]\n",
    "\n",
    "def extract_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in SKILLS_DB:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # check for bi-grams and tri-grams (example: machine learning)\n",
    "    doc=nlp(resume_text)\n",
    "    for token in doc.noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in SKILLS_DB:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddfa1c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_skills(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5ec26fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5250e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AIDA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AIDA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\AIDA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\AIDA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\AIDA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    " \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    " \n",
    "RESERVED_WORDS = [\n",
    "    'school',\n",
    "    'college',\n",
    "    'Institute',\n",
    "    'Ecole',\n",
    "    'Engineering'\n",
    "] \n",
    "\n",
    " \n",
    "def extract_education(input_text):\n",
    "    organizations = []\n",
    " \n",
    "    # first get all the organization names using nltk\n",
    "    for sent in nltk.sent_tokenize(input_text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'ORGANIZATION':\n",
    "                organizations.append(' '.join(c[0] for c in chunk.leaves()))\n",
    " \n",
    "    # we search for each bigram and trigram for reserved words\n",
    "    # (college, university etc...)\n",
    "    education = set()\n",
    "    for org in organizations:\n",
    "        for word in RESERVED_WORDS:\n",
    "            if org.lower().find(word):\n",
    "                education.add(org)\n",
    " \n",
    "    return education\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71d053d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AIDA',\n",
       " 'BOUANIS Data',\n",
       " 'Crowling',\n",
       " 'EDUCATION Engineer',\n",
       " 'HTML',\n",
       " 'INSEA',\n",
       " 'JAVA',\n",
       " 'JavaScript',\n",
       " 'LANGUAGES',\n",
       " 'NVIDIA',\n",
       " 'PHP',\n",
       " 'PROUD OF',\n",
       " 'SQL',\n",
       " 'STRENGTHS',\n",
       " 'Statistics',\n",
       " 'andNumPy'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_education(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
